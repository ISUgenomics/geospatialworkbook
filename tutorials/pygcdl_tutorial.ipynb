{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167f7d6b",
   "metadata": {},
   "source": [
    "# Python Package for USDA-ARS SCINet GeoCDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0826c2",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This tutorial covers the python package `pygcdl` for the SCINet Geospatial Common \n",
    "Data Library (GeoCDL), a community project from the \n",
    "[Geospatial Research Working Group](https://scinet.usda.gov/working-groups/geospatial) \n",
    "to reduce the time and effort to access commonly used geospatial datasets. This \n",
    "tutorial is based on the vignette for the R counterpart of pygcdl, rgeocdl. We have\n",
    "collected several large gridded data products to store on Ceres and created a \n",
    "REST API for SCINet users to request the spatiotemporal subsets of those data \n",
    "that they need. The geospatial processing to create those subsets executes on \n",
    "Ceres and a service node has been setup to serve the API. \n",
    "\n",
    "This python package is a user-friendly interface to pass along user requests to the \n",
    "core GeoCDL API from the compute nodes. That is, the python package does not perform \n",
    "the geospatial processing itself. A major benefit of using this python package is that \n",
    "it was designed to integrate into a user's geospatial data processing workflow in \n",
    "Python. For example, a user storing their study area boundary definition as a \n",
    "`geopandas.GeoDataFrame` object can pass along that object to this package's \n",
    "functions and the package will do the necessary formatting of the data to make it \n",
    "compatible with GeoCDL. \n",
    "\n",
    "The workflows we will cover include uploading a GeoJSON of an LTAR site, \n",
    "requesting data from two datasets clipped to the boundary of the LTAR site \n",
    "with equivalent resolutions and CRSs, and visualizing the resulting \n",
    "maps. We also show how to extract point level information from a gridded layer.\n",
    "\n",
    "This tutorial assumes you are running this notebook in JupyterLab on \n",
    "Ceres. The easiest way to do that is with \n",
    "[Open OnDemand](http://ceres-ood.scinet.usda.gov/). As of this writing, the GeoCDL \n",
    "is only available on Ceres and not Atlas. \n",
    "\n",
    "If you have any questions, problems, or requests related to the python interface, please \n",
    "use the issue tracker on our GitHub repository: \n",
    "[https://github.com/USDA-SCINet/pygcdl](https://github.com/USDA-SCINet/pygcdl). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19763f61",
   "metadata": {},
   "source": [
    "## Nomenclature\n",
    "\n",
    "* <b>GeoCDL:</b> Geospatial Common Data Library, a collection of commonly used raster \n",
    "  datasets accessible from an API running on SCINet's Ceres cluster\n",
    "* <b>Raster:</b> A geospatial datatype where data is stored as a grid of regularly sized pixels. Geospatial rasters contain geospatial metadata, which maps each pixel of the raster to a geospatial location on the Earth's surface. Examples of geospatial raster file types include: geotiff (.tif), and netCDF (.nc).\n",
    "* <b>Vector:</b> A geospatial datatype where data is stored as a collection of points, lines, or polygons. Each coordinate maps to a location on Earth's surface. Examples of geospatial vector file types include: geojson (.geojson), and shapefiles (.shp). \n",
    "* <b>CRS:</b> Coordinate Reference System, also known as a spatial reference system. A\n",
    "  system for defining geospatial coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060e13a",
   "metadata": {},
   "source": [
    "## Data Details\n",
    "\n",
    "###### Dataset: MODIS NDVI\n",
    "* Link: [https://doi.org/10.3334/ORNLDAAC/1299](https://doi.org/10.3334/ORNLDAAC/1299)\n",
    "* Details: This data set provides Moderate Resolution Imaging Spectroradiometer \n",
    "  (MODIS) normalized difference vegetation index (NDVI) data, smoothed and gap-filled, \n",
    "  for the conterminous US for the period 2000-01-01 through 2015-12-31. The data \n",
    "  were generated using the NASA Stennis Time Series Product Tool (TSPT) to generate \n",
    "  NDVI data streams from the Terra satellite (MODIS MOD13Q1 product) and Aqua \n",
    "  satellite (MODIS MYD13Q1 product) instruments. TSPT produces NDVI data that \n",
    "  are less affected by clouds and bad pixels.\n",
    "\n",
    "###### Dataset: PRISM\n",
    "* Link: [https://prism.oregonstate.edu/](https://prism.oregonstate.edu/)\n",
    "* Details: The PRISM Climate Group gathers climate observations from a \n",
    "  wide range of monitoring networks, applies sophisticated quality control \n",
    "  measures, and develops spatial climate datasets to reveal short- and long-term \n",
    "  climate patterns. The resulting datasets incorporate a variety of modeling \n",
    "  techniques and are available at multiple spatial/temporal resolutions, covering \n",
    "  the period from 1895 to the present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f8ae56",
   "metadata": {},
   "source": [
    "## Primary Libraries\n",
    "\n",
    "| Name | Description | Link |\n",
    "|:--|:--|:--|\n",
    "| pygcdl | Python interface for SCINet GeoCDL API | https://github.com/USDA-SCINet/pygcdl |\n",
    "| geopandas | Geospatial vector data for python | https://geopandas.org/en/stable/ |\n",
    "| rioxarray | Geospatial raster data for python | https://corteva.github.io/rioxarray/stable/ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf36866",
   "metadata": {},
   "source": [
    "### Tutorial Steps:\n",
    "0. Import Libraries\n",
    "1. Specify area and dates of interest \n",
    "2. Select datasets and their variables\n",
    "3. Download the data\n",
    "4. Visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a6674",
   "metadata": {},
   "source": [
    "## Part 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1842bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages.\n",
    "import pygcdl\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests as r\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pygcdl object in order to interact with the pygcdl package.\n",
    "pygcdl_obj = pygcdl.PyGeoCDL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d063c",
   "metadata": {},
   "source": [
    "## Part 1: Specify area and dates of interest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997c00b",
   "metadata": {},
   "source": [
    "Here, we specify the spatial extent of our requests. We can request either polygon-based or point-based subsets. When we request a subset, we can specify the spatial extent in one of these three ways:\n",
    "\n",
    "- <b>GUID: </b> Users can use the `upload_geometry()` function to upload a file or `geopandas` GeoDataFrame object, receive a Geometry Upload Identifier (GUID), and use that GUID for subsequent data requests.\n",
    "- <b>Clip: </b> Users can specify the coordinates of a bounding box (polygon data only).\n",
    "- <b>GeoDataFrame: </b> Users can build a `geopandas.GeoDataFrame` object and use it in requests directly, without uploading it in advance.\n",
    "\n",
    "For this tutorial, we will use the `upload_geometry()` function to upload a shapefile containing polygon data that represents the Jornada Experimental Range in southern New Mexico, and use the GUID generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, download the GeoJSON from AgCROS\n",
    "url = \"https://services1.arcgis.com/SyUSN23vOoYdfLC8/arcgis/rest/services/LTAR_Base/FeatureServer/1/query?where=acronym='JER'&f=pgeojson\"\n",
    "response = r.get(url)\n",
    "filename = \"jer_bounds_sf.geojson\"\n",
    "\n",
    "# Save the file locally.\n",
    "with open(filename, mode=\"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82c287",
   "metadata": {},
   "source": [
    "The file `jer_bounds_sf.geojson` that we just downloaded is in EPSG:4326, which is the default for geojson files. Say we wish to download raster data through pygcdl in the CRS EPSG:32613 because EPSG:32613 is a projected CRS that is applicable to our area of interest. (You can learn more about EPSG:32613 [here](https://epsg.io/32613)). In this next block, we read our polygon data into a `geopandas.GeoDataFrame` object, and then transform our `geopandas.GeoDataFrame` object into our desired CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into geopandas.\n",
    "bounds = gpd.read_file(filename)\n",
    "# Transform geopandas dataframe to the desired CRS.\n",
    "bounds = bounds.to_crs(\"EPSG:32613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865ca96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the downloaded boundary.\n",
    "bounds.boundary.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9850",
   "metadata": {},
   "source": [
    "We can see from the plotted map that the site is an irregular shape. For cases like this where the geometry is defined by many points, it is easiest to provide GeoCDL with a file containing the geometry definition, instead of uploading clipping coordinates. We can upload this geodataframe to GeoCDL using the `upload_geometry` function which returns a unique geometry upload identifier (GUID) that we will use later in our subset request. This stand-alone upload step is optional, but recommended if you are likely to submit multiple subset requests with the same geometry so that it is uploaded just once. You could alternatively use the `geopandas` GeoDataFrame object directly in requests instead of using a GUID, but if we upload the geometry file, then we can re-use the same GUID in subsequent requests.\n",
    "\n",
    "Here, we upload our file by calling `upload_geometry()` on our `pygcdl_obj` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guid = pygcdl_obj.upload_geometry(bounds)\n",
    "print(guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90ed70",
   "metadata": {},
   "source": [
    "To finish the spatial component of our subset request, we will define our target spatial resolution and a resampling method. By indicating a target spatial resolution along with our geometry, we are telling GeoCDL that we want a spatially-harmonized 'datacube', which means that each requested data layer has the same CRS, spatial resolution, and spatial extent. \n",
    "\n",
    "Unless we specify otherwise, the target CRS in this case is that of our geometry. Resampling is the process by which the GeoCDL calculates pixel values when the cell grid changes, like when we change the resolution or CRS. This calculation is performed by `rasterio`, and you can find the full list of reprojection methods [here](https://rasterio.readthedocs.io/en/stable/api/rasterio.enums.html#rasterio.enums.Resampling). The default resampling method is to take the nearest pixel's value. Here, we choose the \"bilinear\" method, which you can learn more about [here](https://gisgeography.com/bilinear-interpolation-resampling/).\n",
    "\n",
    "Our CRS, EPSG:32613, is in units of meters. Therefore, our spatial resolution is in units of meters. Here, we specify that we want each pixel to represent an area of 1000 by 1000 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "spat_res = 1000 # in units of meters\n",
    "resample_method = \"bilinear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455a40a",
   "metadata": {},
   "source": [
    "Next, we specify our temporal data. The GeoCDL accepts multiple temporal range formats so that many different user \n",
    "needs can be met. In this example, we are interested in July-August 2008. One way to specify that is with the years and months together as `dates='2008-07:2008-08'` or separately as below. By specifying years and months, we are letting GeoCDL know that we are interested in monthly data. If we only specify years, then it will infer we want annual data and if we also specifies days, then it will infer we want daily data. If the inferred date grain is supported by a dataset, then we use that grain for that dataset. If that date grain is not supported by a dataset, then GCDL uses the \"grain method\" variable, if set, to determine what other grains the user is willing accept. For example, here we specify \"finer\" to indicate that if monthly data is not available, we are also willing to accept daily data. Other options for grain methods include \"strict\", \"skip\", \"any\", and \"coarser\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = \"2008\"\n",
    "months = \"7:8\"\n",
    "grain_method = \"finer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced32c2c",
   "metadata": {},
   "source": [
    "## Part 2: Select datasets and their variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4013a",
   "metadata": {},
   "source": [
    "We can use the `list_datasets()` function to list all of the datasets that are available in the Geospatial Common Data Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba342a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pygcdl_obj.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760e1c0",
   "metadata": {},
   "source": [
    "We can use the `get_dataset_info()` function to learn more about one specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ee0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the MODIS_NDVI dataset.\n",
    "pygcdl_obj.get_dataset_info(\"MODIS_NDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0442c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the PRISM dataset.\n",
    "pygcdl_obj.get_dataset_info(\"PRISM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecaed69",
   "metadata": {},
   "source": [
    "Next, we will specify the datasets and variables we wish to use with a `pandas` dataframe. Here we specify that we want the 'ppt' variable from the PRISM dataset, and the 'NDVI' variable from the MODIS_NDVI dataset. We can format our dataset and variable specifications as a `pandas` dataframe, a numpy array, a matrix, or a dictionary. Here, we format our specifications as a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsvars = pd.DataFrame(\n",
    "    [[\"PRISM\", \"ppt\"], [\"MODIS_NDVI\", \"NDVI\"]], \n",
    "    columns=[\"dataset\", \"variable\"])\n",
    "print(dsvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32697e50",
   "metadata": {},
   "source": [
    "## Part 3: Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56462ad3",
   "metadata": {},
   "source": [
    "First, we create a directory where we would like our data to download to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f40cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"output\")\n",
    "if not output_path.is_dir():\n",
    "    output_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56771dd",
   "metadata": {},
   "source": [
    "Up until now, we have been primarily saving our request specifications as variables. We will now pass each of those variables to the GeoCDL and download our subset using the `download_polygon_subset` function. GeoCDL returns a zipped folder of results and `pygcdl` unzips it for you. `download_polygon_subset` returns the filenames in that folder. We have here two monthly PRISM layers, seven daily MODIS NDVI layers (every 8 days), plus a metadata file that lists metadata related to the geospatial datasets as well as the GeoCDL request itself. The raster layer files are GeoTIFFs with the dataset, variable, and date indicated in the  filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052baee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_files = pygcdl_obj.download_polygon_subset(\n",
    "    dsvars=dsvars, \n",
    "    years=years,\n",
    "    months=months,\n",
    "    grain_method=grain_method,\n",
    "    t_geom=guid,\n",
    "    resolution=spat_res,\n",
    "    dsn=output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750c597",
   "metadata": {},
   "source": [
    "## Part 4: Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d1221",
   "metadata": {},
   "source": [
    "Our use of `pygcdl` for this example is complete, but we can visualize the data that we downloaded. We can see that in our site, NDVI increased over July-August in 2008 but by different degrees within the site. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a0a6e",
   "metadata": {},
   "source": [
    "#### MODIS_NDVI Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the NDVI GeoTIFFs.\n",
    "modis_tifs = [k for k in subset_files if k.endswith('.tif') and \"NDVI\" in k]\n",
    "print(modis_tifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of rioxarray datasets and count the images. Use the\n",
    "# 'mask_and_scale=True' option to read nodata values as nan and to apply the\n",
    "# scale factor.\n",
    "modis_image_stack = [\n",
    "    rioxarray.open_rasterio(k, mask_and_scale=True) for k in modis_tifs\n",
    "]\n",
    "num_images = len(modis_image_stack)\n",
    "print(num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the range of values of NDVI data in the first image.\n",
    "modis_image_stack[0].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5870253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the dates from the filenames and use them to title the subplots.\n",
    "ndvi_layer_names = [\n",
    "    string.split(\"NDVI_NDVI_\")[1].split(\".tif\")[0] for string in modis_tifs\n",
    "]\n",
    "\n",
    "# Plot the data.\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize = (18, 10), sharey=True)\n",
    "fig.suptitle(\"NDVI Values in Jornada Experimental Range, July-August 2028\", fontsize=25)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "for i in range(num_images):\n",
    "    im = modis_image_stack[i].plot(ax=axs[math.floor(i/4), i%4], add_colorbar=False)\n",
    "    axs[math.floor(i/4), i%4].set_title(ndvi_layer_names[i], fontsize=20)\n",
    "    axs[math.floor(i/4), i%4].tick_params(labelrotation=45)\n",
    "axs[1,3].axis(\"off\")\n",
    "label_axis = fig.add_axes([0.79, 0.12, 0.05, 0.3])\n",
    "cbar = fig.colorbar(im, cax=label_axis)\n",
    "cbar.set_label(label=\"NDVI\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e220b09",
   "metadata": {},
   "source": [
    "#### PRISM Visualizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of PRISM GeoTIFFs.\n",
    "prism_tifs = [k for k in subset_files if \".tif\" in k and \"PRISM\" in k]\n",
    "print(prism_tifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f55760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of rioxarray datasets and count the images. Use the\n",
    "# 'masked=True' option to read nodata values as nan. Otherwise, nodata values\n",
    "# for this data are read in as -9999\n",
    "prism_image_stack = [\n",
    "    rioxarray.open_rasterio(k, masked=True) for k in prism_tifs\n",
    "]\n",
    "num_images = len(prism_image_stack)\n",
    "print(num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90329e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Min\", float(prism_image_stack[0].min()))\n",
    "print(\"Max\", float(prism_image_stack[0].max()))\n",
    "print(\"Mean\", float(prism_image_stack[0].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_image_stack[0].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baa38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dates from the filenames and use them to title the subplots.\n",
    "prism_layer_names = [\n",
    "    string.split(\"PRISM_ppt_\")[1].split(\".tif\")[0] for string in prism_tifs\n",
    "]\n",
    "\n",
    "# Plot the data.\n",
    "fig, axs = plt.subplots(ncols=2, figsize = (14, 8), sharey=True)\n",
    "fig.suptitle(\"PRISM Values in Jornada Experimental Range, July-August 2028\", fontsize=25)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "for i in range(num_images):\n",
    "    im = prism_image_stack[i].plot(ax=axs[i], add_colorbar=False)\n",
    "    axs[i].set_title(ndvi_layer_names[i], fontsize=20)\n",
    "    axs[i].tick_params(labelrotation=45)\n",
    "fig.colorbar(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
