{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4efa6d6",
   "metadata": {},
   "source": [
    "<!--\n",
    "---\n",
    "title: Distributing raster calculations with tiles\n",
    "layout: single\n",
    "author: Heather Savoy\n",
    "author_profile: true\n",
    "header:\n",
    "  overlay_color: \"444444\"\n",
    "  overlay_image: /assets/images/margaret-weir-GZyjbLNOaFg-unsplash_dark.jpg\n",
    "---\n",
    "-->\n",
    "\n",
    "**Last Update:** 6 October 2022 <br />\n",
    "**Download Jupyter Notebook**: [GRWG22_RasterTiles.ipynb](https://geospatial.101workbook.org/tutorials/GRWG22_RasterTiles.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d5f01",
   "metadata": {},
   "source": [
    "## Overview\n",
    "A common approach to distribute the workload in raster calculations is to split\n",
    "a large raster into smaller 'tiles' or 'chunks'. This tutorial shows how to \n",
    "speed up a NDVI calculation by creating tiles and then distributing the tiles \n",
    "across cores. This process is not specific to calculating NDVI. You can use this\n",
    "approach with any calculation that is performed per cell/pixel without depending\n",
    "on the data in neighboring pixels (if neighboring data is needed, extra steps are \n",
    "involved).\n",
    "\n",
    "In this tutorial, parallelization is performed within python using the resources \n",
    "allocated to the launched JupyterLab Server session. If you are interested in seeing\n",
    "an example on how to submit your own SLURM job, please see \n",
    "[this tutorial](https://geospatial.101workbook.org/ExampleGeoWorkflows/GRWG22_ZonalStats_wSLURM_python).\n",
    "\n",
    "*Language*: `Python`\n",
    "\n",
    "*Primary Libraries/Packages*:\n",
    "\n",
    "|Name|Description|Link|\n",
    "|-|-|-|\n",
    "| `rioxarray` | rasterio xarray extension | https://corteva.github.io/rioxarray/stable/readme.html |\n",
    "| `rasterio` | access to geospatial raster data | https://rasterio.readthedocs.io/en/latest/ |\n",
    "| `multiprocessing` | Process-based parallelism | https://docs.python.org/3/library/multiprocessing.html |\n",
    "| `dask` | Scale the Python tools you love | https://www.dask.org |\n",
    "\n",
    "## Nomenclature\n",
    "\n",
    "* *Parallel processing:* Distributing computational tasks among multiple cores. \n",
    "* *Core:* A processor on a central processing unit; a physical or logical component \n",
    "  that can execute computational tasks. \n",
    "* *Tile (or chunk):* A continuous segment of a raster dataset or multi-dimensional \n",
    "  array.\n",
    "* *NDVI:* Normalized Difference Vegetation Index, a quantity derived from the red\n",
    "  and near-infrared bands of imagery to detect vegetation. \n",
    "\n",
    "## Analysis Steps\n",
    "* Open Imagery and Setup Tiles - Open a GeoTIFF of Landsat 7 imagery and divide\n",
    "  it into multiple smaller images.\n",
    "* Define NDVI function - Write a function to calculate Normalized Difference \n",
    "  Vegetation Index from an image file.\n",
    "* Compare serial versus parallel computation times - Measure the time it takes to\n",
    "  perform the NDVI calculation across tiles in serial and in parallel\n",
    "  * Option 1: using the `multiprocessing` package\n",
    "  * Option 2: using `Dask`\n",
    "* Visualize NDVI results - View the NDVI values in tiles as a whole image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c9a8b",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries / Packages\n",
    "\n",
    "Below are commands to run to create a new Conda environment named 'geoenv' that contains the packages used in this tutorial series. To learn more about using Conda environments on Ceres, see [this guide](https://scinet.usda.gov/guide/conda/). NOTE: If you have used other Geospatial Workbook tutorials from the SCINet Geospatial Research Working Group Workshop 2022, you may have aleady created this environment and may skip to launching JupyterHub.\n",
    "\n",
    "First, we call `salloc` to be allocated resources on a compute node so we do not burden the login node with the conda installations. Then we load the `miniconda` conda module available on Ceres to access the `Conda` commands to create environments, activate them, and install Python and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4988dd0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "salloc\n",
    "module load miniconda\n",
    "conda create --name geoenv\n",
    "source activate geoenv\n",
    "conda install geopandas rioxarray rasterstats plotnine ipython dask dask-jobqueue -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e0ac6",
   "metadata": {},
   "source": [
    "To have JupyterLab use this conda environment, we will make a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508da8bc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ipython kernel install --user --name=geo_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733a9d5",
   "metadata": {},
   "source": [
    "This tutorial assumes you are running this python notebook file in JupyterLab on \n",
    "Ceres. The easiest way to do that is with \n",
    "[Open OnDemand](http://ceres-ood.scinet.usda.gov/).  Select the following parameter \n",
    "values when requesting a Jupyter: Ceres app to be launched (all other \n",
    "values can be left to their defaults):\n",
    "\n",
    "* `Slurm Partition`: short\n",
    "* `Number of hours`: 1\n",
    "* `Number of cores`: 16\n",
    "* `Memory required`: 24G\n",
    "* `Jupyer Notebook vs Lab`: Lab\n",
    "\n",
    "Once you are in JupyterLab with this notebook open, select your kernel by clicking on the *Switch kernel* button in the top right corner of the editor. A pop-up will appear with a dropdown menu containing the *geo_kernel* kernel we made above. Click on the *geo_kernel* kernel and click the *Select* button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16873d-e1e4-4474-8016-0538b23f8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xarray\n",
    "import rioxarray\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import multiprocessing as mp\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bd376-a27d-4df1-8bab-6236f5be2b87",
   "metadata": {},
   "source": [
    "#### Step 1a: Open imagery\n",
    "\n",
    "The imagery file we will use is available from the `stars` R package. It is an \n",
    "example image from Landsat 7. In its documentation, it indicates that the first three bands are \n",
    "from the visible part of the electromagnetic spectrum (blue, green, red) and the \n",
    "fourth band in the near-infrared band. We can use `rioxarray`'s function `open_rasterio` to\n",
    "read the imagery file. The `plot.imshow` function will let us visualize the image in\n",
    "typical RGB. We can see that it is of a coast with forested areas that should \n",
    "have relatively high NDVI values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7aface-3cba-49af-b2cf-8f38c2f55a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "L7_ETMs = rioxarray.open_rasterio('L7_ETMs.tif').astype('int16')\n",
    "\n",
    "# RGB Image using the third-first bands\n",
    "L7_ETMs[[2,1,0]].plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc892337",
   "metadata": {},
   "source": [
    "This image is fairly small and we do not actually need to speed-up the NDVI \n",
    "calculation with parallel processing. So we will make it artificially a big data \n",
    "problem by disaggregating the pixels, i.e. making a greater number of smaller \n",
    "cells, for the sake of a portable example. The function `rio.reproject` accepts a \n",
    "raster object and a new shape, i.e. a new set of counts for rows and columns. Below, \n",
    "we are using a factor of 20 to disaggregate our raster which \n",
    "will create 400x pixels than the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d327105",
   "metadata": {},
   "outputs": [],
   "source": [
    "disagg_factor = 20\n",
    "new_width = L7_ETMs.rio.width * disagg_factor\n",
    "new_height = L7_ETMs.rio.height * disagg_factor\n",
    "\n",
    "nir_red_fine = L7_ETMs[[2,3]].rio.reproject(\n",
    "    L7_ETMs.rio.crs,\n",
    "    shape=(new_height, new_width),\n",
    "    resampling=Resampling.nearest,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "nir_red_fine.rio.to_raster('nr_fine.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f37426",
   "metadata": {},
   "source": [
    "#### Step 1b: Making tiles\n",
    "\n",
    "First, we will specify how many tiles we want. This example\n",
    "will use 4 tiles in each directions for a total of 16 tiles. Next, we will use\n",
    "nested `for` loops to iterate through our rows and columns of tiles, subset\n",
    "our fine-scale image to that region, and then write the tile to a file with \n",
    "the tile indices indicated in the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb6aa6-cfa8-4fe1-a3ea-45835c041a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tiles = [4,4]\n",
    "\n",
    "tile_width = int(new_width/num_tiles[0])\n",
    "tile_height = int(new_height/num_tiles[1])\n",
    "\n",
    "for i in range(num_tiles[0]):\n",
    "    for j in range(num_tiles[1]):\n",
    "        nir_red_tile = nir_red_fine[:,(i*tile_width):((i+1)*tile_width),(j*tile_height):((j+1)*tile_height)]\n",
    "        nir_red_tile.rio.to_raster('tile_{0}_{1}.tif'.format(i,j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385cba5-1376-4b92-8024-7cbfc8a4e887",
   "metadata": {},
   "source": [
    "<a id='NDVI'></a>\n",
    "### Step 2: Defining NDVI function\n",
    "\n",
    "Next, we want to define a function that will calculate NDVI for the pixels in \n",
    "one tile. The function will accept a tile's filename, open that file with `rioxarray`,\n",
    "calculate NDVI for the pixels, then write the resulting single-band raster to\n",
    "a new file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72176612-c798-421f-bcab-97924a28cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_diff_r(fname):\n",
    "  nr = rioxarray.open_rasterio(fname)\n",
    "  NDVI_r = (nr[1] - nr[0])/(nr[1] + nr[0])\n",
    "  NDVI_r.rio.to_raster('NDVI_' + fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5602072-b3b2-4a78-9c78-abba5e081c00",
   "metadata": {},
   "source": [
    "<a id='serial'></a>\n",
    "### Step 3: Comparing serial and parallel processing times\n",
    "\n",
    "To have a baseline to which to compare how long the parallel approach takes, we\n",
    "can first time the serial approach. The `time.time()` function provides \n",
    "basic time reporting capabilities and when called before and after a computation, we can\n",
    "find the processing time as the difference in number of seconds. \n",
    "\n",
    "### Step 3a: `multiprocessing` \n",
    "\n",
    "#### Serial\n",
    "\n",
    "For our serial approach, we will use a `for` loop to iterate over our tile files and \n",
    "call our `normalized_diff_r` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b72427-64fa-4f92-bfc2-d7614f04daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_files = glob.glob('tile*.tif')\n",
    "tile_files = [os.path.basename(t_f) for t_f in tile_files]\n",
    "\n",
    "st = time.time()\n",
    "for t_f in tile_files:\n",
    "    normalized_diff_r(t_f)\n",
    "    \n",
    "et = time.time()\n",
    "print('Processing time: {:.2f} seconds'.format(et - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae7268-500e-4567-93c4-7c92826fb3fb",
   "metadata": {},
   "source": [
    "#### Parallel\n",
    "\n",
    "To modify the serial approach above, we can use the `multiprocessing` function\n",
    "`Pool` to set how many cores we want to use (here, matching the number of tiles) \n",
    "and the `pool.map_async` function\n",
    "to asynchonously execute `normalized_diff_r` for our tiles across those cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed842bdb-2d97-4939-9874-ee2766fbe87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "pool = mp.Pool(num_tiles[0]*num_tiles[1])\n",
    "results = pool.map_async(normalized_diff_r, tile_files)\n",
    "pool.close()    \n",
    "pool.join()\n",
    "et = time.time()\n",
    "\n",
    "print('Processing time: {:.2f} seconds'.format(et - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f545fc2-76a8-42b3-b68c-a96cc10baa04",
   "metadata": {},
   "source": [
    "### Step 4: Visualize results\n",
    "\n",
    "We have processed each tile, so we can merge the results back and see our NDVI results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8692aa8-7388-4884-93e5-0ded41524777",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_files = ['NDVI_' + t_f for t_f in tile_files]\n",
    "elements = []\n",
    "for file in ndvi_files:\n",
    "    elements.append(rioxarray.open_rasterio(file))\n",
    "\n",
    "merged = xarray.combine_by_coords(elements).squeeze()\n",
    "\n",
    "merged.plot.imshow(cmap='YlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55168b7c-6947-444d-8823-0fca3e5fa6a8",
   "metadata": {},
   "source": [
    "### Step 5: Parallel - Dask\n",
    "\n",
    "Dask is an alternative approach to this problem. The package `rioxarray`, which we used to open our original raster, is a geospatial extension of `xarray`, a package for manipulating multi-dimensional arrays that also has built-in connections with [Dask, a flexible library for parallel computating in Python](https://docs.dask.org/en/stable/). We can use the `xarray`/`Dask` integration to have the tiling work done for us. \n",
    "\n",
    "First, we will tell Dask that we are on a cluster managed by SLURM and pass a few arguments to tell SLURM what kind of specification we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c87151-cae5-4ca8-800d-ccb4b46e6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(cores=2, memory='30GB', queue='short', processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652aa2b",
   "metadata": {},
   "source": [
    "Since we may not need the same number of cores throughout our processing, or we are not quite \n",
    "sure what the optimal number of cores may be, we can take advantage of Dask's option to have \n",
    "an adaptive cluster. The code chunk below tells Dask to request from SLURM 1 to 16 jobs at \n",
    "a given time depending on what we request Dask to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d47628-4fce-4979-aea8-39ecf6c8ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum_jobs=1, maximum_jobs=16)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251475e",
   "metadata": {},
   "source": [
    "To begin using Dask in our NDVI calculation, we will restart by opening our fine-resolution imagery with \n",
    "`rioxarray.open_rasterio`, but this time pass a value to the `chunks` argument. By indicating that we\n",
    "want chunks, we are telling `xarray`/Dask to split up our multi-dimensional array (multi-band image) into\n",
    "chunks, or tiles. If you are not sure how to define the size of your chunks, you can just pass `'auto'` like\n",
    "below and Dask will determine the chunk sizes for you.\n",
    "\n",
    "When you print the Dask array object information to screen, the chunk sizes (in shape and bytes) are \n",
    "displayed along with a visual of your multi-dimensional array shape and the typical `xarray` printed\n",
    "information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44a569-78a8-4620-adc3-e223a69bbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L7_ETMs_fine = rioxarray.open_rasterio('nr_fine.tif', chunks= 'auto')\n",
    "L7_ETMs_fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca084ae",
   "metadata": {},
   "source": [
    "So now we have our imagery open and our chunks defined, we can continue writing our code as \n",
    "we would operate on a typical `rioxarray` object. The key difference is that Dask arrays\n",
    "defer computation until needed. So we can execute the line below to calculate NDVI for the\n",
    "whole image, but only the instructions are stored - the actual calculation has not happened \n",
    "yet. We print the newly created object and see the Dask-format printed output indicating that\n",
    "the result is now a single band, as we expect for NDVI. That is because Dask is keeping track\n",
    "of how our multi-dimensional array *will* be modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b82f-6d08-4622-9e88-9e14a455f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI =  (L7_ETMs_fine[1] - L7_ETMs_fine[0])/(L7_ETMs_fine[1] + L7_ETMs_fine[0])\n",
    "NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf56904",
   "metadata": {},
   "source": [
    "To have the actual calculation be performed, a function requiring the results needs to be called. This can be an explicit request, e.g. the `compute` function below, or implicit, e.g. plotting the array's values. Below, we request for our `NDVI` array to be computed and measure the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161d75c-5b84-490f-ad64-fb0a2a1f98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "NDVI.compute()\n",
    "et = time.time()\n",
    "print('Processing time: {:.2f} seconds'.format(et - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9210e4",
   "metadata": {},
   "source": [
    "During the computation, you can check the SLURM queue by running the `squeue -u firstname.lastname` in a shell with your SCINet username and see the jobs Dask submits to perform the calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e79dd4-921b-4203-8573-85fe79c609ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI.plot.imshow(cmap='YlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b60850-f8aa-41e0-a72f-2f34d55f147f",
   "metadata": {},
   "source": [
    "### Continue to explore\n",
    "\n",
    "Things you can change in this tutorial to see how it affects the difference in\n",
    "processing time between serial and parallel (or the two parallel library \n",
    "options):\n",
    "\n",
    "* The disaggregation factor\n",
    "* The number of tiles\n",
    "* The number of cores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5fd0ed268e7e1b4f4ff0b0f7170b45930872f3469ffae2abb34377fde66ce90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
