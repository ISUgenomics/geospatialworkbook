{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last Update**: Oct 2020 <br />\n",
    "**Jupyter Notebook**: [Tutorial4_Visualizing_and_Modifying_DL_Networks.ipynb](https://geospatial.101workbook.org/tutorials/Tutorial4_Visualizing_and_Modifying_DL_Networks.ipynb)\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ISUgenomics/geospatialworkbook/HEAD?filepath=tutorials)\n",
    "\n",
    "\n",
    "# Tutorial 4: Visualizing and Modifying DL Networks\n",
    "\n",
    "## Laura E. Boucheron, Electrical & Computer Engineering, NMSU\n",
    "\n",
    "### October 2020\n",
    "\n",
    "Copyright (C) 2020  Laura E. Boucheron\n",
    "\n",
    "This information is free; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This work is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this work; if not, If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we pick up with the trained MNIST Network from Tutorial 2 and explore some ways of probing the characteristics of the trained network to help us debug common pitfalls in adapting network architectures.Â  \n",
    "\n",
    "This tutorial contains 5 sections:\n",
    "  - **Section 0: Preliminaries**: some notes on using this notebook, how to download the image dataset that we will use for this tutorial, and import commands for the libraries necessary for this tutorial\n",
    "  - **Section 1: Printing Characteristics of the CNN** how to print textual summaries of the CNN architecture\n",
    "  - **Section 2: Visualizing Activations** how to filter an example image through thhe MNIST network and visualize the activations\n",
    "  - **Section 3: Inputting New and Different Data to the Network** how to process new data to be compatible with the MNIST network and the effects of showing a non-digit image to the network\n",
    "  - **Section 4: The VGG Network** an exploration of the VGG16 network.\n",
    "  \n",
    "There are a few subsections with the heading \"**<span style='color:Green'> Your turn: </span>**\" throughout this tutorial in which you will be asked to apply what you have learned.  \n",
    "\n",
    "Portions of this tutorial have been taken or adapted from https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/ and the documentation at https://keras.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Preliminaries \n",
    "## A Note on Jupyter Notebooks\n",
    "\n",
    "There are two main types of cells in this notebook: code and markdown (text).  You can add a new cell with the plus sign in the menu bar above and you can change the type of cell with the dropdown menu in the menu bar above.  As you complete this tutorial, you may wish to add additional code cells to try out your own code and markdown cells to add your own comments or notes. \n",
    "\n",
    "Markdown cells can be augmented with a number of text formatting features, including\n",
    "  - bulleted\n",
    "  - lists\n",
    "\n",
    "embedded $\\LaTeX$, monotype specification of `code syntax`, **bold font**, and *italic font*.  There are many other features of markdown cells--see the jupyter documentation for more information.\n",
    "\n",
    "You can edit a cell by double clicking on it.  If you double click on this cell, you can see how to implement the various formatting referenced above.  Code cells can be run and markdown cells can be formatted using Shift+Enter or by selecting the Run button in the toolbar above.\n",
    "\n",
    "Once you have completed (all or part) of this notebook, you can share your results with colleagues by sending them the `.ipynb` file.  Your colleagues can then open the file and will see your markdown and code cells as well as any results that were printed or displayed at the time you saved the notebook.  If you prefer to send a notebook without results displayed (like this notebook appeared when you downloaded it), you can select (\"Restart & Clear Output\") from the Kernel menu above.  You can also export this notebook in a non-executable form, e.g., `.pdf` through the File, Save As menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1 Downloading Images\n",
    "Download the `my_digits1_compressed.jpg` and `latest_256_0193.jpg` files available on the workshop webpage.  We will use those images in Sections 3 and 4 of this tutorial.  \n",
    "\n",
    "We will also use the `cameraman.png` and `peppers.png` files that we used in Tutorial 1 and the CalTech101 dataset that we used in Tutorial 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2a Import Necessary Libraries (For users using a local machine)\n",
    "Here, at the top of the code, we import all the libraries necessary for this tutorial.  We will introduce the functionality of any new libraries throughout the tutorial, but include all import statements here as standard coding practice.  We include a brief comment after each library here to indicate its main purpose within this tutorial.\n",
    "\n",
    "It would be best to run this next cell before the workshop starts to make sure you have all the necessary packages installed on your machine.\n",
    "\n",
    "A few other notes:\n",
    " - After the first import of keras packages, you may get a printout in a pink box that states\n",
    "```\n",
    "Using Theano backend\n",
    "```\n",
    "or\n",
    "```\n",
    "Using TensorFlow backend\n",
    "```\n",
    " - You may get one or more warnings complaining about various configs.  As long as you don't get any errors, you should be good to go.  You can, if you wish, fix whatever is causing a warning at a later point in time.  I find it best to copy and paste the error warning itself into a Google search and tack on the OS in which you encountered the error.  Seldom have I encountered an error that someone else hasn't encountered in my same OS.\n",
    " - The third to the last line in the following code cell imports the MNIST dataset.\n",
    " - The last two lines load the VGG16 network and the weights for that network trained on the ImageNet dataset.  The code below will load the VGG16 network, trained on ImageNet.  The first time this code is run, the trained network will be downloaded.  Subsequent times, the trained network will be loaded from the local disk.  This network is very large (528 MB) as we will see shortly, so it may take some time to download.  Generally, we would include the last line below as part of our code rather than imports, but we include it here to allow that download to complete before the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # mathematical and scientific functions\n",
    "import imageio # image reading capabilities\n",
    "import skimage.color # functions for manipulating color images\n",
    "import skimage.transform # functions for transforms on images\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "\n",
    "# format matplotlib options\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "import keras.backend # information on the backend that keras is using\n",
    "from keras.models import Model # a generic keras model class used to modify architectures\n",
    "from keras.utils import np_utils # functions to wrangle label vectors\n",
    "from keras.models import Sequential # the basic deep learning model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D # important CNN layers\n",
    "from keras.models import load_model # to load a pre-saved model (may require hdf libraries installed)\n",
    "from keras.preprocessing.image import load_img # keras method to read in images \n",
    "from keras.preprocessing.image import img_to_array # keras method to convert images to numpy array\n",
    "from keras.applications.vgg16 import preprocess_input # keras method to transform images to VGG16 expected characteristics\n",
    "from keras.applications.vgg16 import decode_predictions # keras method to present highest ranked categories\n",
    "from keras.preprocessing.image import ImageDataGenerator # framework to input batches of images into keras\n",
    "\n",
    "from keras.datasets import mnist # the MNIST dataset\n",
    "from keras.applications import vgg16 # the VGG network\n",
    "model_vgg16 = vgg16.VGG16(include_top=True,weights='imagenet') # download the ImageNet weights for VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2b Build the Conda Environment (For users using the ARS HPC Ceres with JupyterLab)\n",
    "Open a terminal from inside JupyterLab (File > New > Terminal) and type the following commands\n",
    "```\n",
    "source activate\n",
    "wget https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/aiworkshop.yml\n",
    "conda env create --prefix /project/your_project_name/envs/aiworkshop -f aiworkshop.yml\n",
    "```\n",
    "This will build the environment in one of your project directories. It may take 5 minutes to build the Conda environment. \n",
    "\n",
    "See https://kerriegeil.github.io/NMSU-USDA-ARS-AI-Workshops/setup/ for more information.\n",
    "\n",
    "When the environment finishes building, select this environment as your kernel in your Jupyter Notebook (click top right corner where you see Python 3, select your new kernel from the dropdown menu, click select) \n",
    "\n",
    "You will want to do this BEFORE the workshop starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.4 Load Your Trained MNIST Model\n",
    "At the end of Tutorial 3 we saved the trained MNIST model `model1` in `model1.h5`.  Here will load that model and we can pick up right where we left off.\n",
    "\n",
    "If you were not able to save the model at the end of Tutorial 3, you can re-run the training of the MNIST model here before we start the rest of the tutorial.  For your convenience, below is the complete code that will load and preprocess the MNIST data and define and train the model.  You can cut and paste the code here into a code cell in this notebook and run it.\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution2D, MaxPooling2D\n",
    "model1 = Sequential()\n",
    "model1.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model1.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model1.fit(X_train, Y_train, batch_size=64, epochs=1, verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded the trained MNIST model from Tutorial 3.  Since this is a new notebook, however, we do not have the actual MNIST data loaded. We copy the code for loading and preprocessing the MNIST data from the Tutorial 3 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Printing Characteristics of the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The `summary` method\n",
    "The `summary` method of a keras model will display a basic text summary of the CNN architecture with layer name, layer type, output shape, and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about the `None` value in shape\n",
    "The `None` value in the output shapes is used as a placehoder before the network knows how many samples it will be processing.\n",
    "\n",
    "### Using tab-compete to explore attributes and methods\n",
    "The tab-complete feature of `ipython` can be very helpful to explore the available attributes and methods for a variable.  There are useful attributes and methods for the model `model1` and for the layers in the model, accessed with the `layers` attribute of the model.\n",
    "\n",
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Explore the attributes and methods of the model variable `model1` by placing your cursor after the `.` and pressing tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Explore the attributes and methods of the layers in `model1`.  Change the index into `model1.layers` in the first code cell and run that cell to access the different CNN layers.  Then, place your cursor after the `.` and press tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model1.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(layer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A layer-wise summary of input and output shapes\n",
    "While the `summary` method of the model prints some useful information, there are additional pieces of information that can be very useful.  Below is a function definition  which will print a layer-wise summary of the input and output shapes.  This information can be very helpful in helping to understand and debug the workings (or non-workings) of the model.  This code loops over each layer in the model using the `layers` attribute of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(model):\n",
    "    print('Layer Name\\t\\tType\\t\\tInput Shape\\t\\tOutput Shape\\tTrainable')# print column headings\n",
    "    for layer in model.layers:  # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))\n",
    "        elif ltype=='MaxPooling2D': # print for maxpool layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t'+\\\n",
    "                  str(layer.output_shape))\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(layer.input_shape)+'\\t\\t'+\\\n",
    "                  str(layer.output_shape)+'\\t'+str(layer.trainable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a summary of the input and output shapes by passing `model1` to the `print_shapes` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapes(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Does this summary reconcile with the discussion in Tutorial 3 about the architecture of the MNIST model?  You might find it helpful to refer to the Tutorial 3 slides with the visualization of the MNIST network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 A layer-wise summary of filter shape and parameters\n",
    "Below is a function definition which will print a layer-wise summary of the filters and parameters.  This information can also be very helpful in helping to understand and debug the workings (or non-workings) of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model):\n",
    "    total_params = 0 # initialize counter for total params\n",
    "    trainable_params = 0 # initialize counter for trainable params\n",
    "    print('Layer Name\\t\\tType\\t\\tFilter shape\\t\\t# Parameters\\tTrainable') # print column headings\n",
    "    for layer in model.layers: # loop over layers\n",
    "        lname = layer.name # grab layer name\n",
    "        ltype = type(layer).__name__ # grab layer type\n",
    "        ltype[ltype.find('/'):] # parse for only the last part of the string\n",
    "        if ltype=='Conv2D': # print for convolutional layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "        elif ltype=='MaxPooling2D': # print for max pool layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t'+ltype+'\\t---------------\\t\\t---')\n",
    "        elif ltype=='Flatten': # print for flatten layers\n",
    "            print(lname+'\\t\\t'+ltype+'\\t\\t---------------\\t\\t---')\n",
    "        elif ltype=='Dense': # print for dense layers\n",
    "            weights = layer.get_weights()\n",
    "            print(lname+'\\t\\t\\t'+ltype+'\\t\\t'+str(weights[0].shape)+'\\t\\t'+\\\n",
    "                  str(layer.count_params())+'\\t'+str(layer.trainable))\n",
    "            if layer.trainable:\n",
    "                trainable_params += layer.count_params()\n",
    "            total_params += layer.count_params() # update number of params\n",
    "    print('---------------')\n",
    "    print('Total trainable parameters: '+str(trainable_params)) # print total params\n",
    "    print('Total untrainable parameters: '+str(total_params-trainable_params))\n",
    "    print('Total parameters: '+str(total_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a summary of the input and output shapes by passing `model1` to the `print_shapes` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the total number of parameters!!!\n",
    "The MNIST model that we trained yesterday has **more than 600,000 parameters** that it learned during training!  We will see later in this tutorial that is is actually a very small network.\n",
    "\n",
    "We note a few things about the number of parameters per layer:\n",
    "  - The second conv layer has a lot more parameters than the first.  That is due to the fact that the second conv layer filters across all 32 channels of the activations from the first conv layer.  \n",
    "  - The max pool and flatten layers don't have any parameters. \n",
    "  - The fully connected (dense) layers are the source of a large proportion of the total parameters in this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "What implications does the number of trainable parameters per layer have on transfer learning and decisions about which layers to freeze?  There is a code cell below prepopluated with the code to clone `model1` and to freeze all but the last layer for you to modify and explore using the `print_params` and `print_shapes` functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_clone = keras.models.clone_model(model1)\n",
    "model1_clone.set_weights(model1.get_weights())\n",
    "\n",
    "for layer in model1_clone.layers[:-1]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on number of parameters per layer\n",
    "This cell describes a few more details of how you can reconcile the filter shape and the total number of parameters.  The uninterested reader can skip this section without impeding their ability to complete the rest of the tutorial.\n",
    "\n",
    "Recall that a basic neuron has a set of weights and a bias.  The parameters that must be learned in deep learning layers include both the weights and biases.  We break down the computation for convolutional layers and for fully connected (dense) layers.  We will use the same notation as from the Tutorial 3 slides.\n",
    "\n",
    "#### Convolutional layers:\n",
    "Each filter in a convolutional layer has $K\\cdot K\\cdot C$ weights and one bias, where $K$ is the kernel size and $C$ is the number of channels.  Thus we have a total of $M_{conv}\\cdot K\\cdot K\\cdot C$ weights and $M_{conv}$ biases, where $M_{conv}$ is the number of filters in the layer.  This means a total number of trainable parameters of $M_{conv}(K^2C+1)$.\n",
    "\n",
    "#### Fully connected layers:\n",
    "Each node in a fully connected layer is connected to every node in the previous layer and we thus have $M_{FC}^{(i-1)}$ weights and one bias per node, where $M_{FC}^{(i-1)}$ is the number of weights in the previous fully connected (or flattened) layer.  Thus we have a total of $M_{FC}^{(i)}\\cdot M_{FC}^{(i-1)}$ weights and $M_{FC}^{(i)}$ biases, where $M_{FC}^{(i)}$ is the number of nodes in the current fully connected layer.  This means we have a total number of trainable parameters of $M_{FC}^{(i)}(M_{FC}^{(i-1)}+1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 Visualizing Activations\n",
    "In this section we will explore means to visualize the activations in different layers throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Wrangling the example input image dimensions\n",
    "The responses (activations) for each filter in a layer can be computed by sending an example image through the network and requesting that the network report the output at the layer of interest (rather than at the output layer). \n",
    "\n",
    "First, we need to choose an image to filter through the network.  It is this image for which the activations will be computed.  We begin here with the first test image.  Recall that the network expects a tensor in the form samples$\\times28\\times28\\times1$.  In this case, we'll be providing only one sample, so we need our input to be $1\\times28\\times28\\times1$. \n",
    "\n",
    "The following code reshapes the zeroth test image with is shape $28\\times28\\times1$ into a tensor of shape $1\\times28\\times28\\times1$ where the leading dimension of 1 is just wrangling the dimensionality to the samples$\\times28\\times28\\times1$ format expected of an input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example = X_test[0].reshape(1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the image to give us an idea of the appearance of the original image.  This will allow us to better analyze the filtered images that we will see when we plot the activations.  Since we have added some extra dimensions to this image, we use the `np.squeeze` function to remove those dimensions with only one entry before sending it to `plt.imshow`.  In this case, the `np.squeeze` function returns a $28\\times28$ `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.squeeze(X_example),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Explore the dimensionalities of `X_example` relative to `X_test[0]` and `np.squeeze(X_example)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 Modify `model` to output activations after the first conv layer\n",
    "Now, we modify our `model1` to output the activations after the first convolutional layer.  We use the generic `Model` class from `keras` and specify the same inputs as `model1`, but specify the output to be after the zeroth layer.  This is where the `print_shapes` and `print_params` functions can be very helpful to determine which layer you actually want to specify as output. We call this new model `model1_layer0` to designate that it is the same as `model`, but outputing information after layer 0, i.e., the first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer0 = Model(inputs=model1.inputs, outputs=model1.layers[0].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we ask for the prediction of the model for `X_example`, the model will output the activations at the first convolutional layer instead of the activations at the final softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0_activations = model1_layer0.predict(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Based on our textual summaries of this network, we expect that the output should be of shape $1\\times26\\times26\\times32$.  Check the dimensionality and variable type of `layer0_activations`, as well as the intensity range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3 Visualizing the 32 activations of the first conv layer\n",
    "We can loop over the 32 activations and plot each.  `plt.imshow` will, by default, choose an intensity range to match that of the input image.  This can make it difficult to compare between activations: a bright pixel in one image might actually be more activated than in another.  We can force the plots to be on the same scale of intensities by passing the minimum and maximum intensities to `plt.imshow` using the `vmin` and `vmax` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer0_activations.min() # find min intensity for all activations\n",
    "max_int = layer0_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer0_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer0_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer0_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot activations\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some filters that respond to the entire digit, some that respond only the the horizontal stroke of the digit, some that respond only to the vertical stroke, and we may even see some that don't respond at all.  These filters that don't respond may be tuned for shapes (e.g., curves) that don't appear in the digit 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Explore the change in visualization if you do not use the `vmin` and `vmax` options above.  For your convenience, the code from the cell above is copied below for you to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer0_activations.min() # find min intensity for all activations\n",
    "max_int = layer0_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer0_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer0_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer0_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot activations\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4 Modify model to visualize activations after the second conv layer\n",
    "We can look at the activations of the second convolutional layer with a simple modification of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer1 = Model(inputs=model1.inputs, outputs=model1.layers[1].output)\n",
    "layer1_activations = model1_layer1.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer1_activations.min() # find min intensity for all activations\n",
    "max_int = layer1_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer1_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer1_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer1_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot activations\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the second layer filters have gotten more specific in the structures to which they are responding.  This is consistent with what we know about the hierarchical nature of feature learning in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5 Modify model to visualize activations after the max pool layer\n",
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Modify the code above to visualize the output after the max pool layer.  For your convenience, the code from the cell above is copied below for you to modify.  Note--you probably want to define a new variable for this model to avoid overwriting the other models from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer1 = Model(inputs=model1.inputs, outputs=model1.layers[1].output)\n",
    "layer1_activations = model1_layer1.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer1_activations.min() # find min intensity for all activations\n",
    "max_int = layer1_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer1_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer1_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer1_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot activations\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the activations of the max pool layer are simply lower resolution representations of the second convolutional layer activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.6 Modify model to visualize activations of the fully connected layers\n",
    "While the output of the flattened and fully connected layers are not images, we can visualize the activations by treating them like a one-row image.  This can give us some insight into which neurons are responding the most to the digit 7.\n",
    "### The flattened layer\n",
    "Since the dimensions of the flattened layer are $1\\times4608$, we need to \"stretch\" out the pixels to actually be able to see them.  We use the `aspect` parameter in `plt.imshow` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer3 = Model(inputs=model1.inputs, outputs=model1.layers[3].output)\n",
    "layer3_activations = model1_layer3.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer3_activations,cmap='gray',aspect=50) # plot filter coeffs\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization may not be particularly elucidating, but we include it here for the sake of completeness.  Note that this $1\\times4608$ vector of activations is just a reshaping of the $32\\times12\\times12=4608$ pixels in the max pool activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first fully connected layer\n",
    "Since the dimensions of the first fully connected layer is only $1\\times128$, we don't need to mess with the aspect ratio of the `plt.imshow` visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer4 = Model(inputs=model1.inputs, outputs=model1.layers[4].output)\n",
    "layer4_activations = model1_layer4.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer4_activations,cmap='gray') # plot filter coeffs\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization can be interpreted in some sense as some aggregate of features that this layer is cueing on from the max pool layer.  We expect that different of these neurons will activate for different digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second fully connected layer\n",
    "Note that the second fully connected layer is also the softmax output layer.  We leave the axis labels on here for more easy determination of which digit(s) the network is claiming probability for.  We also put grid lines on the image to even better delineate the different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_layer5 = Model(inputs=model1.inputs, outputs=model1.layers[5].output)\n",
    "layer5_activations = model1_layer5.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer5_activations,cmap='gray') # plot filter coeffs\n",
    "#plt.axis('off')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the output here is a very high confidence in the digit 7 and very little in other digits.  This is consistent with the interpretation of the softmax output layer if we were to look at the actual probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer5_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Explore the activations of the network for other input images.  For your convenience, code cells from above have been copied here for you to modify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a specific input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example = X_test[0].reshape(1,28,28,1)\n",
    "print('Original image')\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(X_example),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First convolutional layer')\n",
    "model1_layer0 = Model(inputs=model1.inputs, outputs=model1.layers[0].output)\n",
    "layer0_activations = model1_layer0.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer0_activations.min() # find min intensity for all activations\n",
    "max_int = layer0_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer0_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer0_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer0_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot filter coeffs\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Second convolutional layer')\n",
    "model1_layer1 = Model(inputs=model1.inputs, outputs=model1.layers[1].output)\n",
    "layer1_activations = model1_layer1.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "min_int = layer1_activations.min() # find min intensity for all activations\n",
    "max_int = layer1_activations.max() # find max intensity for all activations\n",
    "subplot_rows = np.ceil(np.sqrt(layer1_activations.shape[-1])) # determine subplots rows\n",
    "for f in range(0,layer1_activations.shape[-1]): # loop over filters\n",
    "    plt.subplot(subplot_rows,subplot_rows,f+1) # choose current subplot\n",
    "    plt.imshow(np.squeeze(layer1_activations[:,:,:,f]),cmap='gray',\\\n",
    "               vmin=min_int,vmax=max_int) # plot filter coeffs\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flattened layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The flattened layer')\n",
    "model1_layer3 = Model(inputs=model1.inputs, outputs=model1.layers[3].output)\n",
    "layer3_activations = model1_layer3.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer3_activations,cmap='gray',aspect=50) # plot filter coeffs\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First fully connected layer')\n",
    "model1_layer4 = Model(inputs=model1.inputs, outputs=model1.layers[4].output)\n",
    "layer4_activations = model1_layer4.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer4_activations,cmap='gray') # plot filter coeffs\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second fully connected layer (the output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Second fully connected layer (output layer)')\n",
    "model1_layer5 = Model(inputs=model1.inputs, outputs=model1.layers[5].output)\n",
    "layer5_activations = model1_layer5.predict(X_example)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(layer5_activations,cmap='gray') # plot filter coeffs\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 Inputting New and Different Data to the Trained Network\n",
    "In this section, we'll explore the use of this trained network to operate on new data.  We will use the `my_digits1_compressed.jpg` image provided as part of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = imageio.imread('my_digits1_compressed.jpg')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that image is an RGB image of 10 handwritten digits.  You will wrangle this image into a format suitable for input to the MNIST network in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "\n",
    "You should have noticed that image is an RGB image of 10 handwritten digits.  Use what you have learned in Tutorials 1, 2 and 3 to extract each of those 10 digits from the image and get it in the correct form to input to the MNIST network.\n",
    "\n",
    "As a reminder you probably want to pay attention to:\n",
    "  - RGB versus gray\n",
    "  - Variable type\n",
    "  - Intensity range (Hint--you can invert the intensities and have light digits on a dark background by subtracting the image from the maximum intensity.)\n",
    "  - Cropping indices (Hint--rows 295 through 445 and columns 1160 through 1310 will crop the digit 0)\n",
    "  - Resizing\n",
    "  - Correct tensor dimensions: recall that the network expects a tensor in the form samples$\\times28\\times28\\times1$.  In this case, you'll be providing only one sample, so you will need your input to be $1\\times28\\times28\\times1$.\n",
    "\n",
    "Use your extracted digits as input to the MNIST network `model1`.  Does the network predict the correct label for the digit?  What does the predicted softmax label tell you about the confidence in the prediction for this new image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small demonstration of the ability for this network to correctly classify data from an entirely new source.  We note, however, that the preparation of the data is critical for this success.  If we were to pre-process the data in a manner not designed for the MNIST network, we might get very different results.  \n",
    "\n",
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Repeat the above analysis, but keep the digit as dark on a light background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By keeping the background light and the digit dark, the network interprets the background as the digit.  It is not actually processing the information in the same way we interpret this image as the digit we see.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: The VGG16 Network\n",
    "In this section, we will use what we have learned about deep learning and image processing to explore a common CNN network called VGG16.  The VGG network is described in this paper: https://arxiv.org/abs/1409.1556 and is a common architecture for image classification.  This network was trained to classify 1000 categories (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  \n",
    "\n",
    "Note--VGG16 is some 528 MB to download.  This is a typical size for state-of-the-art CNNs.  We actually downloaded these weights when we imported libraries (local machines) or activated the conda environment (HPC).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.1 Loading the VGG Network Trained on ImageNet\n",
    "`keras` includes functions to load a VGG16 network with additional options to download a \"pretrained\" network--i.e., one that has been trained on ImageNet.  ImageNet is a database of millions of images (see http://www.image-net.org/) spanning thousands of categories.\n",
    "\n",
    "The code below will load the VGG16 network, trained on ImageNet.  The first time this code is run, the trained network will be downloaded.  Subsequent times, the trained network will be loaded from the local disk.  This network is very large as we will see shortly, so it may take some time to download.\n",
    "\n",
    "Similar to how we saved our MNIST model at the end of Tutorial 3 and loaded it at the beginning of this tutorial, we are loading a VGG16 model that has already been trained on the millions of images and 1000 categories of ImageNet.  It is no trivial task to train a network the size of VGG16 (weeks on a multiple GPUs), so we want to leverage the work that has already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = vgg16.VGG16(include_top=True,weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `print_shapes` and `print_params` to explore the structure of the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapes(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(model_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 138 million parameters in this network!!!  It has many more layers than the simple MNIST network that we have been working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.2: Classification Capabilities of the VGG16 Network\n",
    "So, what happens if we show a new image to this network?  Let's see what happens if we show it the `cameraman.png` image.  \n",
    "This example is adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e.\n",
    "\n",
    "In the code below, we are leveraging many built-in `keras` functions, including\n",
    "  - `load_img` from `keras` to read in an image while resizing to the expected input size of $224\\times224$\n",
    "  - `preprocess_input` specific to the VGG16 model in `keras` to scale the intensities to the expected range.  I'm unsure of the details of this, but I do know that it's a process more complicated than a simple intensity scaling (since it results in negative intensities).  Further details are likely in the VGG paper (https://arxiv.org/abs/1409.1556), but can be difficult to find sometimes.\n",
    "  - `decode_predictions` specific to the VGG16 model in `keras` to find the top three confidences and map those to the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('cameraman.png', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# predict the probability across all output classes\n",
    "yhat = model_vgg.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "for k in range(0,3):\n",
    "    labelk = label[0][k]\n",
    "    # print the classification\n",
    "    print('%s (%.2f%%)' % (labelk[1], labelk[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the network's performance on this image is pretty decent, specifying that it believes the image is one of a \"tripod.\"  There is not a \"cameraman\" category in ImageNet (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), so the network chose the most likely category from the available ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we repeat the above for the `peppers.png` image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('peppers.png', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# predict the probability across all output classes\n",
    "yhat = model_vgg.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "for k in range(0,3):\n",
    "    labelk = label[0][k]\n",
    "    # print the classification\n",
    "    print('%s (%.2f%%)' % (labelk[1], labelk[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the network's performance on this model is also very good, specifying that the image is of a \"bell pepper\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the cameraman and peppers image contain objects similar to those that the VGG16 network encountered in the ImageNet database.  As such, it does a remarkably good job of classifying those images.  \n",
    "\n",
    "What happens if the network encounters something really different than what it's seen before?  What if you show it an image of something not included in the 1000 classes of objects?  The image `latest_256_0193.jpg` image is an image of the Sun at a wavelength of 193 angsgtroms from NASA's Solar Dynamics Observatory satellite (https://sdo.gsfc.nasa.gov/data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('latest_256_0193.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "# predict the probability across all output classes\n",
    "yhat = model_vgg.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "label = decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "for k in range(0,3):\n",
    "    labelk = label[0][k]\n",
    "    # print the classification\n",
    "    print('%s (%.2f%%)' % (labelk[1], labelk[2]*100))\n",
    "\n",
    "I = imageio.imread('latest_256_0193.jpg')\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(I)\n",
    "plt.title('latest_256_0193.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is torn between classifying this image of the sun as a \"tick\", \"French loaf\", or a \"nail\".  Hmmm.... Things aren't looking so good anymore.  But we must remember that we never showed the network ground truth of the sun in 193 angstroms during training.  We can't really expect that it can jump to that conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "What class does the VGG16 network think your data belong to?  If you don't have data with you, you can peruse the internet for images of something that you want to try classifying with the network.  Just save the image to the same directory as this notebook and use the code above to classify it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Using an image of your choice, use the methods we learned above to explore the workings of the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Transfer Learning on the VGG16 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of how to perform transfer learning on the VGG16 architecture using the CalTech101 dataset as the new input data.\n",
    "\n",
    "Similar to how we modified the model to output activations at certain layers bove, we can truncate the VGG16 model to any desired layer and then add on additional layers at our discretion.  In this case, since we noted relatively good performance of the basic VGG16 architecture on images of similar appearance to the object categories in ImageNet, we expect that we won't need to change too much of the VGG16 architecture.  In this example, we choose to modify only the final prediction layer.\n",
    "\n",
    "It is also common practice to retrain on all the fully connected layers.  Generally speaking, the further in appearance your data are from the ImageNet images, the further back in the architecture you probably want to retrain.  \n",
    "\n",
    "In the following code, we keep the entire VGG16 architecture up until the last fully connected layer (which also happens to be the output layer) and define a new model `model2_vgg` which consists only of those layers we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_vgg = Model(inputs=model_vgg.input,outputs=model_vgg.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add in at least a new prediction layer as the final layer.  We could also add additional layers within the network if we thought they were needed.  Since the CalTech101 dataset has 101 classes, we need the final fully connected layer to have 101 nodes and a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = model2_vgg.output # take the output as currently defined\n",
    "new_output = Dense(101,activation='softmax')(new_output) # operate on that output with another dense layer\n",
    "model2_vgg = Model(inputs=model2_vgg.input,outputs=new_output) # define a new model with the new output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we have defined a new architecture, where we amputated the final fully connected layer and stitched back on a new one.  If we look at the layers of this new model using the modified `print_params` function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(model2_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that all the layers have attribute `trainable` set to `True`.  This means that if we were to train this new model, we will train all 138 million parameters.  We don't want to do this.\n",
    "\n",
    "We want to \"freeze\" the parameters for all layers except that new one that we added.  To do this, we set the `trainable` attribute of all layers we wish to freeze to `False`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model2_vgg.layers[:-1]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we check the trainability of the layers, we see that all layers except the final layer are frozen, and we will be training (learning) only some 413,797 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_params(model2_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to point out new architecture at the CalTech101 data in order to train that final layer.  We again use built-in functions in `keras` to handle the flow of data through the network.  With MNIST we could fit all the training images in one array in memory and grab batches from there.  With a larger dataset like CalTech101, we begin to lose our ability to fit everything in memory and instead will read image from the specified directory into batches at training time.\n",
    "\n",
    "The code below uses an `ImageDataGenerator` class from keras and defines the preprocessing applied to that image as the `preprocess_input` function we already used above.  Recall that this is a preprocessing function defined specifically for the VGG16 network.\n",
    "\n",
    "Next, we use the `flow_from_directory` function of the `ImageDataGenerator` to define a flow of images from a specified directory. It is assumed that the specified directory contains a subdirectory for each class.  Other options specified for this function are\n",
    "  - `target_size` which specifies the spatial dimensions to which all input data will be resized\n",
    "  - `color_mode` which specifies that these images should be treated as RGB images.  These built-in functions are nice to use since they (often, not always) have niceties associated with taking care of things like converting grayscale images to the correct dimensionality.\n",
    "  - `batch_size` the number of images to process per batch in the training\n",
    "  - `class_mode` which specifies that this is a multi-class classification problem\n",
    "  - `shuffle` which specifies that the batches will be selected randomly rather than in alphanumerical order\n",
    "  \n",
    "There are other options available, see `help(train_datagen.flow_from_directory)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory('101_ObjectCategories',\\\n",
    "                                                    target_size=(224,224), color_mode='rgb',\\\n",
    "                                                    batch_size=32, class_mode='categorical',\\\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile the model and specify the same options as we used for the MNIST network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_vgg.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model.  Since we don't have the entire training data to feed to the training, we instead invoke the `fit_generator` method which can utilize the `train_generator` we define above.  Additionally, the `fit_generator` takes a `steps_per_epoch` option rather than a `batch_size` option.  We define the `steps_per_epoch` as the number of images over which we will train divided by the batch size.\n",
    "\n",
    "This code will take a while.  It took about 30 minutes per epoch on 6 4.8 GHz Intel i9 processors.  In most applications, we would train this over multiple epochs to boost the accuracy even higher.  Here we train for one epoch to limit the computational time.  After one epoch, the network reported accuracy above 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_generator.n//train_generator.batch_size # the // does a floor after division\n",
    "model2_vgg.fit_generator(generator=train_generator, steps_per_epoch=step_size_train, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style='color:Green'> Your turn: </span>**\n",
    "Using an image of your choice from CalTech101, use the methods we learned above to explore the workings of the new transfer-learned VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
